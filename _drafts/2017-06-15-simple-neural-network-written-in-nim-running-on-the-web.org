#+STARTUP: showall
#+OPTIONS: toc:nil -:nil
---
layout: post
title: Simple neural network written in nim running on the web
tags: nim projects
comments: true
---

I've made a simple neural network in Nim based on a fantastic book by Tariq Rashid. Read on if you want some implementation details and a working demo.

---

* The book
#+ATTR_HTML: :float left
#+ATTR_HTML: :style max-width: 50%
http://i.imgur.com/UC1meTs.jpg

Great book! This is a very nice introduction to neural networks. This book not only goes over the theory but also the step by step implementation. At the end you will have a simple neural network that recognizes handwritten numbers, all done with Python. This was the perfect candidate for practicing my nim-fu because this project has a low number of dependencies, relative simple math and clear, readable Python code.

The book starts gently talking about predicting machines, goes on explaining what a neuron means in the context of a neural network, why matrices are useful, what are weights and how does the actual learning process even work. Following chapters deal with the actual implementation, some python theory and extra things you can do with the nn once you finished the project.

I will not go into many details about neural networks, Tariq Rashid does a much much better job, I will just give you hight overview of the working of the demo.

* Overview
First of we require some inputs nodes(neurons), the network needs to `see` the image, each node will represent a pixel from the image, since we use 28 by 28 pixel image size, this means we need a total of *784* input nodes. 

The `hidden` nodes are the nodes where the actual learning process really happens, each node connects with other nodes, but not all connections are the same, the importance of a connection is determined by it's weight, higher weight means greater importance. The learning process just changes the weights between the nodes to get closer and closer to the correct answer. More hidden layers we have, the more learning we can do.

Lastly we need to formulate an answer, since we deal only with numbers ranging from 0 to 9 we can use *10* output nodes, each one representing a digit, the biggest value of a node determines the `winning` node, the answer.

You can picture our neural network looking similar to this:
http://i.imgur.com/ckrX2V8.png

Now that we have the network in place, in order for it to be useful we must train the 'beast'. For that I have used(as instructed) a dataset containing handwritten numbers, each having a corresponding label(the right answer). This is a sample taken from the collection. http://i.imgur.com/oE8sJF7.png You can check the whole data set [[https://pjreddie.com/projects/mnist-in-csv/][here]] .

The learning process is as follows: let's say that the answer given by the network is ~out~, the correct answer is ~correct~, so we get ~error = (correct - out) * learning rate~. The error resulted will tell us how close or far we are from the correct answer, learning rate determines the size of steps we take towards minimizing the error, a small learning rate means a fine grained approach. Choosing a good learning rate is important because is we chose one that is to small, the learning process is inefficient, the steps we take towards the goal are to small, going to big we risk overshooting and miss the answer altogether.

With the resulting error we must adjust each node's weight. We do that by starting from the output nodes going in reverse towards the input nodes(back propagation). The math involved in achieving this is called [[https://en.wikipedia.org/wiki/Gradient_descent][gradient descent]] .

* Implementation

In my case python brings home the bacon, for me working in a language that I enjoy was always my primary objective. While I will not ditch Python any time soon, I chose to implement all my side projects in Nim because either I want to learn about some low level stuff like: pointers, memory management(if you opt out from nim's garbage collector), concepts, etc or simply because I need/want raw speed without to much of a hassle. I like to play for time to time with a compiled, statically typed language.

Here is where Nim shines, I get the same fun factor as with Python without the speed penalty.

For implementing the neural network I chose a very cool matrix library called [[https://unicredit.github.io/neo/][neo]]. Neo's goal is to become what numpy is for python programmers. It make use of libraries such as ATLAS, OpenBlas, Intel MKL, or with GPU support using NVIDIA CUDA 8.0

Neo's documentation and api where nice to work with and as a bonus I got multi-core support right of the bat. Training my neural network went smooth and fast.

Once I finished the project I wanted to demo it on the web. Nim's javascript back-end comes in handy, instead of compiling to C just chose javascript. Since Neo uses ATLAS/OpenBlas... libraries, I had to chose another matrix library without external dependencies. Performance was not critical because on the web since I wanted to have a pre-trained network, so [[https://github.com/twist-vector/matrix][this]] module fit the bill nicely.

For the HTML5 canvas bindings I chose [[https://github.com/define-private-public/HTML5-Canvas-Nim][HTML5-Canvas-Nim]]. Looking at the source code you clearly see how little work is needed to interface with javascript from nim. Using HTML5-Canvas-Nim I got a working demo in no time.

To make things more easy for the neural network I decided to use a bounding box around the drawing and resize only that to 28x28 pixels. This means that you can draw in any possition and almost any size and the network still gets a nice input image to work with. A demo of the code I have `stolen` is [[http://phrogz.net/tmp/canvas_bounding_box.html][here]] .

All the source code used is located here: [[https://github.com/bontavlad/neural_nim][neural nim]]. Branch ~master~ is where I have used Neo module, and branch ~js_matrix~ is used for the web demo part. ~web~ branch is where I tried to experiment with WebAssembly. I got to the point where I compile to WebAssembly and it works, but I don't know yet how to work with wasm generated classes in javascript.


* Demo:
 #+BEGIN_EXPORT html
<script type="text/javascript" src="/js/neural_nim/main.js"></script>
<style type="text/css">
  canvas {
      border: 1px solid black;
      margin: 0px;
  }
</style>
<canvas id="surface" width="500" height="500"></canvas>
<h3>I think you drawn: <span id="sure"> --- </span></h3>
<h4>But it could be also: <span id="maybe"> --- </span></h4>
<button id="clear-btn" type="button">Clear canvas</button>
<button id="guess-btn" type="button">Guess!</button>
<button id="correct-btn" type="button">Correct Me!</button>
<input type="text" id="correct-input" name="usrname" size="1">
 #+END_EXPORT
